{
    "collab_server" : "",
    "contents" : "---\ntitle: \"The Drought Dataset\"\nauthor: <span style=\"color:#16a085\">Christopher K. Wikle & Dan Pagendam</span>\noutput: ioslides_presentation\n---\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = FALSE)\n```\n\n\n## The Drought Dataset\n- The data for this exercise consits of:\n    - monthly grids (2 degree x 2 degree) of sea surface temperature (SST) anonaly.\n    - monthly rainfall anomaly in mm for the Murray Darling Basin (MDB).\n- The data was obtained from two sources:\n    - http://www.bom.gov.au/climate/change/\n    - http://iridl.ldeo.columbia.edu/\n- We'll attempt to use a <span style=\"color:#16a085\">Long Short-Term Memory (LSTM)</span> Model to obtain 3 month out forecasts of rainfall anomaly from SST grids.\n\n\n\n## Required Packages\n- For this exerice you will require the following packages:\n    - <span style=\"color:#16a085\">raster</span>\n- You can install and load these as follows:\n```{r echo = TRUE}\n#install.packages(\"raster\")\nlibrary(raster)\nlibrary(deepLearningR)\nlibrary(keras)\n```\n\n\n\n## Load the Drought Data \n\n```{r echo = TRUE}\ndata(drought)\n```\n\n\n\n## Visualising the SST Anomaly Data\n\n```{r fig1, fig.height = 5, fig.width = 8, fig.align = \"center\", echo = FALSE}\n  plot(anomalyRasterList[[1]], main = paste0(\"Sea Surface Temperature Anomaly (\", dateGrid[1, 1], \"/\", dateGrid[1, 2], \")\"), xlab = \"Longitude\", ylab = \"Latitude\", zlim = c(-5, 5))\n```\n\n## Visualising the SST Anomaly Data\n\n```{r fig2, fig.height = 5, fig.width = 8, fig.align = \"center\", echo = FALSE}\n  plot(anomalyRasterList[[2]], main = paste0(\"Sea Surface Temperature Anomaly (\", dateGrid[2, 1], \"/\", dateGrid[2, 2], \")\"), xlab = \"Longitude\", ylab = \"Latitude\", zlim = c(-5, 5))\n```\n\n## Visualising the SST Anomaly Data\n\n```{r fig3, fig.height = 5, fig.width = 8, fig.align = \"center\", echo = FALSE}\n  plot(anomalyRasterList[[3]], main = paste0(\"Sea Surface Temperature Anomaly (\", dateGrid[3, 1], \"/\", dateGrid[3, 2], \")\"), xlab = \"Longitude\", ylab = \"Latitude\", zlim = c(-5, 5))\n```\n\n\n## Visualising the SST Anomaly Data\n\n```{r fig4, fig.height = 5, fig.width = 8, fig.align = \"center\", echo = FALSE}\n  plot(anomalyRasterList[[4]], main = paste0(\"Sea Surface Temperature Anomaly (\", dateGrid[4, 1], \"/\", dateGrid[4, 2], \")\"), xlab = \"Longitude\", ylab = \"Latitude\", zlim = c(-5, 5))\n```\n\n\n\n## Visualising the SST Anomaly Data\n\n```{r fig5, fig.height = 5, fig.width = 8, fig.align = \"center\", echo = FALSE}\n  plot(anomalyRasterList[[5]], main = paste0(\"Sea Surface Temperature Anomaly (\", dateGrid[5, 1], \"/\", dateGrid[5, 2], \")\"), xlab = \"Longitude\", ylab = \"Latitude\", zlim = c(-5, 5))\n```\n\n\n## Visualising the SST Anomaly Data\n\n```{r fig6, fig.height = 5, fig.width = 8, fig.align = \"center\", echo = FALSE}\n  plot(anomalyRasterList[[6]], main = paste0(\"Sea Surface Temperature Anomaly (\", dateGrid[6, 1], \"/\", dateGrid[6, 2], \")\"), xlab = \"Longitude\", ylab = \"Latitude\", zlim = c(-5, 5))\n```\n\n\n## Data Manipulation\n\n```{r echo = TRUE}\nbatchSize <- 32\nforecastMonthsAhead <- 3\ntimestepsPerSample <- 24\ntrainingInds <- 1:1300\ntestInds <- 1301:1434\n```\n- We will reduce the dimensionality of the rasters using <span style=\"color:#16a085\">singular value decomposition </span>.\n- We will project the 2772 pixels onto 100 <span style=\"color:#16a085\">Empirical Orthogonal Functions (EOFs)</span>.\n```{r echo = TRUE}\nnumComponents <- 100\nEOFList <- rasterToEOFs(anomalyRasterList[trainingInds], \n           numComponents = numComponents, plot = FALSE)\nv.train <- EOFList[[\"rasterEOFs\"]][[\"v.dim.red\"]]\n```\n\n## Dimension Reduction\n\n```{r echo = FALSE}\nEOFList <- rasterToEOFs(anomalyRasterList[trainingInds], numComponents = numComponents, plot = TRUE)\n```\n\n\n## Dimension Reduction\n```{r echo = FALSE}\nvalidPixels <- EOFList[[\"raster.validPixels\"]]\neof1 <- EOFsToRaster(matrix(EOFList$rasterEOFs$EOFs[, 1], ncol = 1),\n        matrix(rep(1, 1), nrow = 1), c(33, 84),\n        validPixels)[[1]]\nextent(eof1) <- extent(anomalyRasterList[[1]])\nplot(eof1, main = \"1st EOF\", xlab = \"Longitude\", ylab = \"Latitude\")\n```\n\n## Dimension Reduction\n```{r echo = FALSE}\neof2 <- EOFsToRaster(matrix(EOFList$rasterEOFs$EOFs[, 2], ncol = 1),\n        matrix(rep(1, 1), nrow = 1), c(33, 84),\n        validPixels)[[1]]\nextent(eof2) <- extent(anomalyRasterList[[1]])\nplot(eof2, main = \"2nd EOF\", xlab = \"Longitude\", ylab = \"Latitude\")\n```\n\n## Dimension Reduction\n```{r echo = FALSE}\neof3 <- EOFsToRaster(matrix(EOFList$rasterEOFs$EOFs[, 3], ncol = 1),\n        matrix(rep(1, 1), nrow = 1), c(33, 84),\n        validPixels)[[1]]\nextent(eof3) <- extent(anomalyRasterList[[1]])\nplot(eof3, main = \"3rd EOF\", xlab = \"Longitude\", ylab = \"Latitude\")\n```\n\n\n## Dimension Reduction\n```{r echo = FALSE}\neof100 <- EOFsToRaster(matrix(EOFList$rasterEOFs$EOFs[, 100], ncol = 1),\n        matrix(rep(1, 1), nrow = 1), c(33, 84),\n        validPixels)[[1]]\nextent(eof100) <- extent(anomalyRasterList[[1]])\nplot(eof100, main = \"100th EOF\", xlab = \"Longitude\", ylab = \"Latitude\")\n```\n\n\n## Checking the Dimension Reduction\n```{r echo = TRUE}\ntestSample <- 1434\nX <- EOFList$rasterEOFs$EOFs\nr1 <- anomalyRasterList[[testSample]]\nvalidPixels <- EOFList[[\"raster.validPixels\"]]\nY <- getValues(r1)\nY <- Y[validPixels]\nlm1 <- lm(Y~X)\nintercept <- coefficients(lm1)[1]\nalpha <- coefficients(lm1)[1]\nbeta <- coefficients(lm1)[2:(numComponents + 1)]\nr2 <- alpha + EOFsToRaster(X, matrix(beta, nrow = 1),\n      c(33, 84), validPixels)[[1]]\nextent(r2) <- extent(r1)\n```\n\n\n## Checking the Dimension Reduction\n- We can accurately reproduce the SST anomaly grids from the 100 EOFs.\n```{r fig7, fig.height = 3, fig.width = 8, fig.align = \"center\", echo = TRUE}\npar(mfrow = c(1, 2))\nplot(r1, xlab = \"Longitude\", ylab = \"Latitude\", zlim = c(-5, 5))\nplot(r2, xlab = \"Longitude\", ylab = \"Latitude\", zlim = c(-5, 5))\n```\n\n\n## Dimension Reduction of the Test Data\n- Here we <span style=\"color:#16a085\">project the SST anomaly grids in the test set onto the EOFs</span> that we generated from the training data.\n- You can think of <span style=\"color:#16a085\">v.test</span> as a multivariate time series of coefficients that we can use to reconstruct SST anomaly from the EOFs.\n- We can think of these as <span style=\"color:#16a085\">latent features</span> derived from the observed system.\n```{r echo = TRUE}\nv.test <- proj.raster.EOFs(anomalyRasterList[testInds],\n         EOFList[[\"rasterEOFs\"]][[\"EOFs\"]],\n         EOFList[[\"raster.validPixels\"]])\n```\n\n\n## Dimension Reduction of the Test Data\n```{r echo = TRUE, results = \"hide\", fig.show = 'hide'}\npar(mfrow = c(3, 1), mar = c(4,4,1,1))\nplot(trainingInds, v.train[, 1], ty = \"l\", xlim = c(0, 1434),\n     xlab = \"Months\", ylab = \"Variable 1\")\nlines(testInds, v.test[, 1], col = \"blue\")\nlegend(\"topleft\", legend = c(\"train\", \"test\"), horiz = TRUE,\n       box.lwd = 0, col = c(\"black\", \"blue\"), lty = 1)\nplot(trainingInds, v.train[, 2], ty = \"l\", xlim = c(0, 1434),\n     xlab = \"Months\", ylab = \"Variable 2\")\nlines(testInds, v.test[, 2], col = \"blue\")\nlegend(\"topleft\", legend = c(\"train\", \"test\"), horiz = TRUE,\n       box.lwd = 0, col = c(\"black\", \"blue\"), lty = 1)\nplot(trainingInds, v.train[, 3], ty = \"l\", xlim = c(0, 1434),\n     xlab = \"Months\", ylab = \"Variable 3\")\nlines(testInds, v.test[, 3], col = \"blue\")\nlegend(\"topleft\", legend = c(\"train\", \"test\"), horiz = TRUE,\n       box.lwd = 0, col = c(\"black\", \"blue\"), lty = 1)\n```\n\n\n## Dimension Reduction of the Test Data\n```{r fig8, fig.height = 4.5, fig.width = 8, fig.align = \"center\", echo = FALSE}\npar(mfrow = c(3, 1), mar = c(4,4,1,1))\nplot(trainingInds, v.train[, 1], ty = \"l\", xlim = c(0, 1434),\n     xlab = \"Months\", ylab = \"Variable 1\")\nlines(testInds, v.test[, 1], col = \"blue\")\nlegend(\"topleft\", legend = c(\"train\", \"test\"), horiz = TRUE,\n       box.lwd = 0, col = c(\"black\", \"blue\"), lty = 1)\nplot(trainingInds, v.train[, 2], ty = \"l\", xlim = c(0, 1434),\n     xlab = \"Months\", ylab = \"Variable 2\")\nlines(testInds, v.test[, 2], col = \"blue\")\nlegend(\"topleft\", legend = c(\"train\", \"test\"), horiz = TRUE,\n       box.lwd = 0, col = c(\"black\", \"blue\"), lty = 1)\nplot(trainingInds, v.train[, 3], ty = \"l\", xlim = c(0, 1434),\n     xlab = \"Months\", ylab = \"Variable 3\")\nlines(testInds, v.test[, 3], col = \"blue\")\nlegend(\"topleft\", legend = c(\"train\", \"test\"), horiz = TRUE,\n       box.lwd = 0, col = c(\"black\", \"blue\"), lty = 1)\n```\n\n\n## Wrangling Data for a RNN in Keras\n- The predictors are the <span style=\"color:#16a085\">coefficients for the EOFs</span> over time.\n```{r echo = TRUE}\nv.combined <- rbind(v.train, v.test)\n```\n- Scale the predictors to the model.\n```{r echo = TRUE}\nv.scaling.train <- scaleCols.pos(v.combined[trainingInds, ])\nv.train.scaled <- v.scaling.train[[\"X.scaled\"]]\nv.scaling.test <- scaleCols.pos(v.combined[testInds, ],\n                  colMaxsX = v.scaling.train[[\"colMaxsX\"]], colMinsX = v.scaling.train[[\"colMinsX\"]])\nv.test.scaled <- v.scaling.test[[\"X.scaled\"]]\n\nv.scaled <- rbind(v.train.scaled, v.test.scaled)\n```\n\n\n## Formatting Tensors for a RNN in Keras\n```{r echo = TRUE}\nnumDims <- ncol(v.scaled)\ntensorData <- tensorfyData.rnn(v.scaled, forecastMonthsAhead,\n              timestepsPerSample, indicesX = 1:numDims,\n              indicesY = 1:numComponents, indicesTrain = trainingInds,\n              indicesTest = testInds, method = \"deterministic\")\nstr(tensorData)\n```\n\n## Scaling Data for a RNN in Keras\n- Scale the Outputs of the Model\n```{r echo = TRUE}\nY.train.inds <- tensorData$y.train.tsInds\nY.test.inds <- tensorData$y.test.tsInds\nY.train.rnn_MDB <- rainfallAnomaly[Y.train.inds, 3]\nY.test.rnn_MDB <- rainfallAnomaly[Y.test.inds, 3]\nY.train.min <- min(Y.train.rnn_MDB)\nY.train.max <- max(Y.train.rnn_MDB)\n\nY.train.rnn_MDB <- (Y.train.rnn_MDB - Y.train.min)/\n                    (Y.train.max - Y.train.min)\nY.test.rnn_MDB <- (Y.test.rnn_MDB - Y.train.min)/\n                    (Y.train.max - Y.train.min)\n```\n\n## Editing the Tensors for the RNN\n- By default, <span style=\"color:#16a085\">tensorfyData.rnn</span> will assume that you are trying to predict the same time series that you are using as inputs.\n- In our case, the <span style=\"color:#16a085\">EOF coefficients are the inputs</span>, but we are attempting to <span style=\"color:#16a085\">forecast rainfall anomaly</span>.\n- So let's replace the output tensors in the list called <span style=\"color:#16a085\">tensorData</span>.\n```{r echo = TRUE}\ntensorData[[\"Y.train.rnn\"]] <- Y.train.rnn_MDB\ntensorData[[\"Y.test.rnn\"]] <- Y.test.rnn_MDB\n```\n\n## Editing the Tensors for the RNN\n- Let's extract the important tensors from the tensor list.\n```{r echo = TRUE}\nX.rnn.train <- tensorData[[\"X.train.rnn\"]]\nX.rnn.test <- tensorData[[\"X.test.rnn\"]]\n\nY.rnn.train <- tensorData[[\"Y.train.rnn\"]]\nY.rnn.test <- tensorData[[\"Y.test.rnn\"]]\n```\n\n\n## A Custom Loss Function\n- We will use a <span style=\"color:#16a085\">Gaussian likelihood function</span> and use the negative log-likelihood as our loss function.\n```{r echo = TRUE}\nGaussian_logLikelihood <- function(y_true, y_pred)\n{\n  K <- backend()\n  muMask <- K$constant(c(1, 0), shape = c(2, 1))\n  sigmaMask <- K$constant(c(0, 1), shape = c(2, 1))\n\n  sigma <- K$exp(K$dot(y_pred, sigmaMask))\n  mu <- K$dot(y_pred, muMask)\n  \n  ll <- -0.5*K$square((mu - y_true)/(sigma)) - K$log(sigma)\n  -1*K$sum(ll, axis = 1L)\n}\n```\n\n## Building an LSTM Model\n```{r echo = TRUE}\nlibrary(keras)\nmodel <- keras_model_sequential()\nmodel %>%\n  layer_lstm(units = 128,\n             input_shape = c(timestepsPerSample, numDims),\n             return_sequences = FALSE,\n             stateful = FALSE) %>%\n  layer_dense(units = 128, activation = \"relu\") %>%\n  layer_dense(units = 2)\n```\n\n\n## Compiling the Model\n- We compile the model with our custom <span style=\"color:#16a085\">negative log-likelihood loss</span>.\n- We will use the <span style=\"color:#16a085\">Adam</span> optimisation algorithm with default parameters.\n```{r echo = TRUE}\nmodel %>% compile(loss = Gaussian_logLikelihood,\n                  optimizer = optimizer_adam())\n```\n\n\n## Training and Early Stopping\n```{r echo = TRUE}\nhistory <- model %>% fit(x = X.rnn.train, y = Y.rnn.train, \n           batch_size = batchSize, epochs = 200, shuffle = TRUE, \n           validation_data = list(X.rnn.test, Y.rnn.test),\n           callbacks = list(callback_early_stopping(\n           monitor = \"val_loss\", min_delta = 0, patience = 20), \n           callback_model_checkpoint(filepath = \"MDB_Gaussian.hd5\",\n           save_best_only = TRUE, save_weights_only = FALSE)))\n\nbestModel <- load_model_hdf5(filepath = \"MDB_Gaussian.hd5\",\n             custom_objects = list(Gaussian_logLikelihood = \n             Gaussian_logLikelihood))\n```\n\n## Assessing the Preditions\n- Calculate the mean and standard deviations of the 3 month out (Gaussian) predictive distributions.\n- Then create 50% and 95% prediction intervals.\n```{r echo = TRUE}\nlstmPredictions <- bestModel %>% predict(X.rnn.test)\n\nmu <- Y.train.min + lstmPredictions[, 1]*(Y.train.max - Y.train.min)\nsigma <- exp(lstmPredictions[, 2])*(Y.train.max - Y.train.min)\nn <- length(mu)\nupper95 <- mu + 1.96*sigma\nlower95 <- mu - 1.96*sigma\nupper50 <- mu + 0.674*sigma\nlower50 <- mu - 0.674*sigma\n```\n\n## Assessing the Preditions\n- Plot the true time series with 3-month-out forecast and 50% and 95% prediction intervals.\n```{r echo = TRUE, results = \"hide\", fig.show = 'hide'}\nplot(rainfallAnomaly[Y.test.inds, 3], ty = \"l\", \n     xlab = \"Time (months)\", \n     ylab = \"MDB Rainfall Anomaly (mm)\")\nlines(mu, col = \"blue\")\npolygon(x = c(1:n, rev(1:n), 1),\n        y = c(lower95, rev(upper95), lower95[1]),\n        col = fade(\"blue\", 100), border = NA)\npolygon(x = c(1:n, rev(1:n), 1),\n        y = c(lower50, rev(upper50), lower50[1]),\n        col = fade(\"blue\", 100), border = NA)\n```\n\n## Assessing the Preditions\n- Plot the true time series with 3-month-out forecast and 50% and 95% prediction intervals.\n```{r echo = FALSE, fig9, fig.height = 4, fig.width = 8, fig.align = \"center\"}\nplot(rainfallAnomaly[Y.test.inds, 3], ty = \"l\",\n     xlab = \"Time (months)\",\n     ylab = \"MDB Rainfall Anomaly (mm)\")\nlines(mu, col = \"blue\")\npolygon(x = c(1:n, rev(1:n), 1),\n        y = c(lower95, rev(upper95), lower95[1]),\n        col = fade(\"blue\", 100), border = NA)\npolygon(x = c(1:n, rev(1:n), 1),\n        y = c(lower50, rev(upper50), lower50[1]),\n        col = fade(\"blue\", 100), border = NA)\n```\n\n## Assessing the Preditions\n- Calculate what percentage of the time the true rainfall anomaly was within the 50% and 95% prediction intervals.\n```{r echo = TRUE}\nn <- (length(Y.test.inds))\ncoverage50 <- length(which(rainfallAnomaly[Y.test.inds, 3]> lower50\n              & rainfallAnomaly[Y.test.inds, 3] < upper50))/n\ncoverage95 <- length(which(rainfallAnomaly[Y.test.inds, 3] > lower95\n              & rainfallAnomaly[Y.test.inds, 3] < upper95))/n\nprint(coverage50)\nprint(coverage95)\n```\n\n\n## Some things to try\n- How does varying the number of units in the LSTM layer affect the predictions?\n- How do the predictions change if you add three dense layers after the LSTM layer (instead of just 1)?\n- How are the predictions if much fewer EOFs are used for prediction?\n- How does fewer EOFs affect the number of parameters in the model?\n\n\n",
    "created" : 1574814505139.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3095867916",
    "id" : "9540C708",
    "lastKnownWriteTime" : 1574814621,
    "last_content_update" : 1574814621542,
    "path" : "~/Dropbox/Deep_Learning_Short_Course/CourseSlides/Exercise 5 - Drought Dataset/Drought.Rmd",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 5,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}